# Building and Deploying AI agents using Crew AI and Amazon Bedrock AgentCore

This is a sample project showing how to build an AI agent with Crew AI and deploy in production using AgentCore.

## About CrewAI

CrewAI is a multi-agent AI framework that enables the creation of collaborative AI agents that work together to solve complex tasks. It provides a structured approach to building AI crews where specialized agents can be assigned specific roles, collaborate through shared memory, and execute tasks in parallel or sequentially. CrewAI simplifies the orchestration of multiple AI agents, making it easy to build sophisticated AI systems that can handle complex workflows and decision-making processes.

## Installing Dependencies

This project uses [uv](https://github.com/astral-sh/uv), a fast Python package installer and resolver. If you don't have uv installed, follow the [installation instructions](https://github.com/astral-sh/uv#installation) on the project's GitHub repository.

Install all project dependencies:

```sh
uv sync
```

I run the following command to install `crewai` CLI:

```sh
uv tool install crewai
```

I previously used the `crewai` CLI to create a new project (`crewai create crew agentcore-crew-ai`). When asked by the tool, I selected one of the Amazon Bedrock models, but it's not a requirement for using AgentCore Runtime.

As described in the Python project file (`pyproject.toml`), the `crew run` command invokes the `run` function in the `main.py` file.

To run in AgentCore Runtime, I need an entrypoint function that extracts request or input parameters from the input payload and invokes the agent crew. The entrypoint function returns the content of the generated report.

First, I install the AgentCore SDK and Starter Toolkit:

```sh
uv add bedrock-agentcore bedrock-agentcore-starter-toolkit
```

Then I add the code in `main.py` to import the AgentCore SDK and add the entrypoint function. To simplify testing locally, I also add at the end a couple of lines to run the `app`. In fact, I can now run it with:

```sh
uv run src/agentcore_crew_ai/main.py
```

## Memory Configuration

Set up AgentCore memory (do this only once for all projects):

```sh
cd ../scripts
uv sync
uv run create-memory
uv run add-sample-memory
cd ../agentcore-crew-ai
```

The `create-memory` script creates a new AgentCore memory instance with all three strategies (User Preferences, Semantic Facts, Session Summaries) and saves the configuration to `../config/memory-config.json`. The `add-sample-memory` script adds a sample memory event ("I like apples but not bananas") to demonstrate the memory system.

**Note**: You only need to run these memory setup steps once. The memory configuration will be shared across all AgentCore framework projects.

After creating the memory, copy the memory configuration files to your project directory:

```sh
cp ../config/memory-config.json .
```

This ensures the memory configuration is available in the container when deployed with AgentCore.

## Deploy using the Amazon Bedrock AgentCore starter toolkit

The `agentcore` command is in the Python virtual environment. I either run it with `uv run agentcore` or activate the virtual environment to just use `agentcore`. I use the latter.

```sh
source .venv/bin/activate
agentcore --help
```

Now I follow the usual process to configure and launch (locally first, then deploying to the cloud) the agent in the AgentCore Runtime.

To solve an issue with a dependency (`chroma-hnswlib`) that needs to be rebuilt with newer versions of Python, I update the base image of the `Dockerfile` generated by `agentcore configure` to use Python 3.11. This issue might be solved by a future update of Crew AI and its dependencies.

```dockerfile
FROM ghcr.io/astral-sh/uv:python3.11-bookworm-slim
```

The `.dockerignore` file skips copying `.env` to the container image. That's a good practice. In my case, I need to pass the `MODEL` environment variable, so I add it explicitly in the `Dockerfile`:

```dockerfile
ENV MODEL=bedrock/us.amazon.nova-pro-v1:0
```

Because AgentCore runs the crew in a container, I don't need to write an output file but return the result back from the entrypoint function. I comment out the line in the `crew.py` file that includes an output file:

```python
#           output_file='report.md'
```

```sh
agentcore configure -n crewaiagent -e src/agentcore_crew_ai/main.py # All default values when asked
# Update Dockerfile to use `FROM ghcr.io/astral-sh/uv:python3.11-bookworm-slim` for the base image
# And `ENV MODEL=bedrock/us.amazon.nova-pro-v1:0` to use the model of my choice
agentcore launch --local
agentcore invoke --local '{ "prompt": "AI multi-agent architectures - Also, what did I say about fruit?" }'
```

When I am satisfied with the local tests, I deploy to the cloud:

```sh
agentcore launch
```

If you modify the code, you can update a cloud deployment by running `agentcore launch` again. It'll create a new version for the same endpoint.

```sh
agentcore status
agentcore invoke '{ "prompt": "AI multi-agent architectures - Also, what did I say about fruit?" }'
```

Before invoking the agent, I use in another terminal the AWS CLI command shown by `agentcore launch` to follow the logs on Amazon CloudWatch Logs:

```sh
aws logs tail /aws/bedrock-agentcore/runtimes/<AGENT_ID-ENDPOINT_ID> --follow
```
